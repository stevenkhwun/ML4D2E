{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Opinions and Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding How Machines Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = 'The quick brown fox jumps over the lazy dog.'\n",
    "text_2 = 'My dog is quick and can jump over fences.'\n",
    "text_3 = 'Your dog is so lazy that it sleeps all the day.'\n",
    "corpus = [text_1, text_2, text_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0]\n",
      " [1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "vectorizer = text.CountVectorizer(binary=True)\n",
    "vectorizer.fit(corpus)\n",
    "vectorized_text = vectorizer.transform(corpus)\n",
    "print(vectorized_text.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 19, 'quick': 15, 'brown': 2, 'fox': 7, 'jumps': 11, 'over': 14, 'lazy': 12, 'dog': 5, 'my': 13, 'is': 8, 'and': 1, 'can': 3, 'jump': 10, 'fences': 6, 'your': 20, 'so': 17, 'that': 18, 'it': 9, 'sleeps': 16, 'all': 0, 'day': 4}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and Enhancing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 1 0 0 2 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "text_4 = 'A black dog just passed by but my dog is brown.'\n",
    "corpus.append(text_4)\n",
    "vectorizer = text.CountVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "vectorized_text = vectorizer.transform(corpus)\n",
    "print(vectorized_text.todense()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     brown: 0.095\n",
      "       dog: 0.126\n",
      "        my: 0.095\n",
      "        is: 0.077\n",
      "     black: 0.121\n",
      "      just: 0.121\n",
      "    passed: 0.121\n",
      "        by: 0.121\n",
      "       but: 0.121\n",
      "\n",
      "Summed values of a phrase: 1.0\n"
     ]
    }
   ],
   "source": [
    "TfidF = text.TfidfTransformer(norm='l1')\n",
    "tfidf_mtx = TfidF.fit_transform(vectorized_text)\n",
    "\n",
    "phrase = 3 # choose a number from 0 to 3\n",
    "\n",
    "total = 0\n",
    "for word in vectorizer.vocabulary_:\n",
    "    pos = vectorizer.vocabulary_[word]\n",
    "    value = list(tfidf_mtx.toarray()[phrase])[pos]\n",
    "    if value !=0:\n",
    "        print (\"%10s: %0.3f\" % (word, value))\n",
    "        total += value\n",
    "print('\\nSummed values of a phrase: %0.1f' % total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the quick': 30, 'quick brown': 24, 'brown fox': 3, 'fox jumps': 9, 'jumps over': 15, 'over the': 21, 'the lazy': 29, 'lazy dog': 17, 'my dog': 19, 'dog is': 7, 'is quick': 11, 'quick and': 23, 'and can': 1, 'can jump': 6, 'jump over': 14, 'over fences': 20, 'your dog': 31, 'is so': 12, 'so lazy': 26, 'lazy that': 18, 'that it': 27, 'it sleeps': 13, 'sleeps all': 25, 'all the': 0, 'the day': 28, 'black dog': 2, 'dog just': 8, 'just passed': 16, 'passed by': 22, 'by but': 5, 'but my': 4, 'is brown': 10}\n"
     ]
    }
   ],
   "source": [
    "bigrams = text.CountVectorizer(ngram_range=(2, 2))\n",
    "print(bigrams.fit(corpus).vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'sam', 'swim', 'time']\n",
      "[[1 0 1 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Luca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Luca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\Luca\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    # Tokenizing\n",
    "    tokens = word_tokenize(text)\n",
    "    # Removing stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Stemming\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vocab = ['Sam loves swimming so he swims all the time']\n",
    "vect = text.CountVectorizer(tokenizer=tokenize)\n",
    "vec = vect.fit(vocab)\n",
    "\n",
    "sentence1 = vec.transform(['George loves swimming too!'])\n",
    "\n",
    "print(vec.get_feature_names())\n",
    "print(sentence1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Textual Datasets from the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.request as urllib2\n",
    "\n",
    "wiki = \"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\"\n",
    "header = {'User-Agent': 'Mozilla/5.0'} \n",
    "query = urllib2.Request(wiki, headers=header)\n",
    "page = urllib2.urlopen(query)\n",
    "soup = BeautifulSoup(page, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find(\"table\", { \"class\" : \"wikitable sortable\" })\n",
    "final_table = list()\n",
    "\n",
    "def extract_txt(cell):\n",
    "    \"\"\"Extracting only text\"\"\"\n",
    "    cells = [c.strip() for c in cell.findAll(text=True) if '[' not in c]\n",
    "    return ' '.join(cells).strip()\n",
    "\n",
    "def filter_sq(txt):\n",
    "    \"\"\"Extracting squared meter values\"\"\"\n",
    "    return txt.split('sq')[0].strip()\n",
    "\n",
    "cols = [extract_txt(cell) for cell in table.findAll(\"th\")]\n",
    "columns = [cols[1], cols[2], cols[3], cols[4], cols[6]]\n",
    "\n",
    "for row in table.findAll('tr'):\n",
    "    cells = row.findAll(\"td\")\n",
    "    if len(cells)>0:\n",
    "        final_table.append([extract_txt(cells[1]), \n",
    "                            extract_txt(cells[2]), \n",
    "                            extract_txt(cells[3]), \n",
    "                            extract_txt(cells[4]), \n",
    "                            filter_sq(extract_txt(cells[6]))\n",
    "                           ])\n",
    "        \n",
    "df = pd.DataFrame(final_table, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          City       State 2019 estimate 2010 Census 2016 land area\n",
      "0     New York    New York     8,336,817   8,175,133          301.5\n",
      "1  Los Angeles  California     3,979,576   3,792,621          468.7\n",
      "2      Chicago    Illinois     2,693,976   2,695,598          227.3\n",
      "3      Houston       Texas     2,320,268   2,100,263          637.5\n",
      "4      Phoenix     Arizona     1,680,992   1,445,632          517.6\n"
     ]
    }
   ],
   "source": [
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scoring and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = 'https://github.com/lmassaron/datasets/releases/'\n",
    "filename += 'download/1.0/shakespeare_lines_in_plays.feather'\n",
    "shakespeare = pd.read_feather(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=1.0, min_df=3, stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(shakespeare.lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=999,\n",
       "    n_components=10, random_state=101, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=n_topics, max_iter=999, random_state=101)\n",
    "nmf.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0: thou thy thee love ll\n",
      "Topic # 1: enter exeunt exit scene ii\n",
      "Topic # 2: king thy lord warwick henry\n",
      "Topic # 3: caesar antony brutus cassius shall\n",
      "Topic # 4: antipholus dromio chain sir syracuse\n",
      "Topic # 5: page ford sir master mistress\n",
      "Topic # 6: rome marcius titus lavinia andronicus\n",
      "Topic # 7: lord good shall sir know\n",
      "Topic # 8: cassio iago desdemona othello moor\n",
      "Topic # 9: hector troilus achilles ajax troy\n"
     ]
    }
   ],
   "source": [
    "def print_topic_words(features, topics, top=5):\n",
    "    for idx, topic in enumerate(topics):\n",
    "        words = \" \".join([features[i] \n",
    "                for i in topic.argsort()[:(-top-1):-1]])\n",
    "        print(f\"Topic #{idx:2.0f}: {words}\")\n",
    "\n",
    "print_topic_words(vectorizer.get_feature_names(), nmf.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Othello act:5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "index = shakespeare.play + ' act:' + shakespeare.act\n",
    "\n",
    "def find_top_match(model, data, index, topic_no):\n",
    "    topic_scores = model.transform(data)[:, topic_no]\n",
    "    best_score = np.argmax(topic_scores)\n",
    "    print(index[best_score])\n",
    "\n",
    "find_top_match(nmf, tfidf, index, topic_no=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing reviews from e-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = 'https://github.com/lmassaron/datasets/'\n",
    "filename += 'releases/download/1.0/imdb_50k.feather'\n",
    "reviews = pd.read_feather(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conquerer of Shamballa shows what happens when creators of an Anime fail to understand what their fans want. I as a fan did not want a 1920's Evil Nazi movie. What I would have liked to see is a real final showdown between Ed and Dante, as we don't REALLY know what became of her. I also would have liked to get Ed back to his world much sooner and have him stay there, to finally get a chance to be normal. You know, raise a family with a certain blonde mechanic, that sort of thing. No, instead I got a convoluted plot involving Nazi mystics, Fritz Lang and about ten minutes of Al, a joke of a Cameo by Roy Mustang and only one Armstrong joke, one short joke and no Winry hitting Ed with a wrench. Above all, it just didn't feel like Fullmetal Alchemist to me.\n"
     ]
    }
   ],
   "source": [
    "print(reviews.review.sample(1).values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reviews.sample(30000, random_state=42)\n",
    "sampled_idx = train.index\n",
    "valid = (reviews[~reviews.index.isin(train.index)]\n",
    "         .sample(10000, random_state=42))\n",
    "sampled_idx.append(valid.index)\n",
    "test = reviews[~reviews.index.isin(sampled_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train.review)\n",
    "\n",
    "def tokenize_and_pad(df, tokenizer, maxlen):\n",
    "    sequences = tokenizer.texts_to_sequences(df.review)\n",
    "    pad = keras.preprocessing.sequence.pad_sequences\n",
    "    padded_seqs = pad(sequences, maxlen)\n",
    "    return padded_seqs, df.sentiment.values\n",
    "\n",
    "X, y = tokenize_and_pad(train, tokenizer, maxlen=256)\n",
    "Xv, yv = tokenize_and_pad(valid, tokenizer, maxlen=256)\n",
    "Xt, yt = tokenize_and_pad(test, tokenizer, maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Luca\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Luca\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\Luca\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 256, 8)            790040    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 792,089\n",
      "Trainable params: 792,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "voc = len(tokenizer.index_word) + 1\n",
    "feats = 8\n",
    "seq_len = 256\n",
    "model.add(keras.layers.Embedding(voc, feats, \n",
    "                          input_length=seq_len))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "30000/30000 [==============================] - 36s 1ms/sample - loss: 0.4355 - acc: 0.7931 - val_loss: 0.2805 - val_acc: 0.8866\n",
      "Epoch 2/2\n",
      "30000/30000 [==============================] - 49s 2ms/sample - loss: 0.1940 - acc: 0.9273 - val_loss: 0.2608 - val_acc: 0.8943\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=2, batch_size=16, \n",
    "                    validation_data=(Xv, yv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.89455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = (model.predict(Xt)>=0.5).astype(int)\n",
    "test_accuracy = accuracy_score(yt, predictions)\n",
    "print(f\"Accuracy on test set: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
